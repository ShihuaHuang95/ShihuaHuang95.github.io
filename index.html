<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shihua Huang</title>
  
  <meta name="author" content="Shihua Huang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon_clipart.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shihua Huang (黄世华)</name>
              </p>
              <p>
                Shihua Huang is currently a PhD candidate with the Dept. of Computing at Hong Kong Polytechnic University, Hong Kong, China.
                </p>
              <p>
                His research interests are in the field of representation learning, notably neural architecture search, deep learning assisted evolutionary algorithms,
                and in particular dense image prediction (auto-driving and medical image analysis).
              </p>
              <p style="text-align:center">
                <a href="mailto:shihuahuang95@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/ShihuaHuangCV.pdf">CV</a> &nbsp/&nbsp
                <a href="data/ShihuaHuang-bio.txt">Bio</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=YVZLfBUAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/ShihuaHuang95/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/shihua_huang.jpg"><img style="width:80%;max-width:100%" alt="profile photo" src="images/shihua_huang_circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
          </td>
        </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <div class="col-12 col-md-0 order-md-12 align-self-center">
          <ul>
            <li> <b>07/2021:</b> Our paper titled "<a href="http://www.shihuahuang.cn/fapn" target='_blank'>FaPN: Feature-aligned Pyramid Network for Dense Image Prediction</a>"
              is accepted to <a href="http://iccv2021.thecvf.com/home" target='_blank'>IEEE ICCV'21. </a> Moreover, our FaPN improved the best <a href="https://arxiv.org/abs/2107.06278" target='_blank'>MaskFormer</a> for <a href="https://paperswithcode.com/sota/semantic-segmentation-on-ade20k" target='_blank'> ADE20k-150  </a>
              by about 1% and achieved 56.7% mIoU, which is the <span style="color:#FF0000; font-weight: bold;"> 2nd </span>  best among all published methods. Many thanks to all co-authors for their kind help.
            </li>

            <li> <b>07/2021:</b> Our paper titled "<a href="https://arxiv.org/pdf/2009.06193" target='_blank'>RelativeNAS: Relative Neural Architecture Search via Slow-Fast Learning</a>"
              is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385" target='_blank'>IEEE T-NNLS. </a>
              Congratulations to <a href="https://scholar.google.com/citations?user=Dad3F3AAAAAJ&hl=en" target='_blank'>Hao Tan</a> for leading the paper.
            </li>

            <li> <b>04/2021:</b> Our paper titled "<a href="https://www.sciencedirect.com/science/article/pii/S2210650221000559"
                                                      target='_blank'>Efficient Evolutionary Neural ArchitectureSearch by Modular Inheritable Crossover.</a>"
              is accepted to <a href="https://www.journals.elsevier.com/swarm-and-evolutionary-computation" target='_blank'>Elsevier SWEVO. </a>
              Congratulations to <a href="https://scholar.google.com/citations?user=I28xMFQAAAAJ&hl=zh-CN" target='_blank'>Cheng He</a> for leading the paper.
            </li>

            <li> <b>04/2020:</b> Our team (<strong>EMI_VR</strong>) achieved the <span style="color:#FF0000; font-weight: bold;"> best result</span> on Video Deblurring Track of <a href="https://competitions.codalab.org/competitions/22235#learn_the_details"
                                                                                                                                                                    target='_blank'>NTIRE 2020 Challenge</a> and our solution is presented as a technical report titled "<a
                    href="https://openaccess.thecvf.com/content_CVPRW_2020/html/w31/Nah_NTIRE_2020_Challenge_on_Image_and_Video_Deblurring_CVPRW_2020_paper.html"
                    target='_blank'>NTIRE 2020 Challenge on Image and Video Deblurring</a>",
              which is accepted to <a href="https://openaccess.thecvf.com/CVPR2020_workshops/CVPR2020_w31" target="_blank"> IEEE CVPR'20 workshop.</a>
            </li>

            <li> <b>03/2020:</b> Our paper titled "<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9082904" target='_blank'>Evolutionary Multi-Objective Optimization Driven by Generative Adversarial Networks (GANs)</a>
              is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221036" target="_blank"> IEEE T-CYB.</a>
              Congratulations to <a href="https://scholar.google.com/citations?user=I28xMFQAAAAJ&hl=zh-CN" target='_blank'>Cheng He</a> for leading the paper.</li>
            </li>

            <li> <b>1/2020:</b> Our proposal titled "Deep Learning Based Aerofoil Design" is approved by Chinese Ministry of Industry and Information Technology (RMB 3,200,000).
            </li>

            <li> <b>11/2019:</b> Our team (<strong>只想划水</strong>) won the <span style="color:#FF0000; font-weight: bold;">  final champion and received 20K RMB reward</span> at
              <a href="https://www.datafountain.cn/competitions/366/ranking?isRedance=0&sch=1450&stage=B" target='_blank'>5th NAVINFO Cup on AutoDriving</a>.
            </li>

            <li> <b>4/2019:</b> Our team (<strong>9102</strong>) ranked <span style="color:#FF0000; font-weight: bold;">  1st place</span> in the first round over <a href="https://tianchi.aliyun.com/competition/entrance/231701/rankingList/2" target='_blank'>Defense Track</a>
              at <a href="https://tianchi.aliyun.com/competition/entrance/231701/introduction" target='_blank'>IJCAI-19 Alibaba Adversarial AI Challenge on Defense</a>.
            </li>

            <li> <b>03/2019:</b> A simple and English version of my undergraduate thesis titled "<a href="https://arxiv.org/pdf/1903.07360" target='_blank'>IvaNet: Learning to jointly detect and segment objets with the help of Local Top-Down Modules</a>
              is released to arXiv. Many thanks for <a href="http://school.freekaoyan.com/ln/neu/daoshi/2016/05-03/1462279859570036.shtml" target='_blank'>Prof. Lu Wang's</a> supervision.
            </li>

            <li> <b>09/2018:</b> Our paper titled "<a href="https://link.springer.com/chapter/10.1007/978-3-030-03341-5_6" target='_blank'>A local top-down module for object detection with multi-scale features</a>"
              is accepted to <a href="https://link.springer.com/book/10.1007/978-3-030-03398-9" target="_blank"> PRCV.</a>
            </li>

          </ul>
        </div>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/fpn_vs_fapn.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://www.shihuahuang.cn/fapn">
                <papertitle>FaPN: Feature-aligned Pyramid Network for Dense Image Prediction</papertitle>
              </a>
              <br>
              <strong>Shihua Huang</strong>,
              <a href="http://www.luzhichao.com/">Zhichao Lu</a>,
              <a href="https://scholar.google.com/citations?user=bjeIdlcAAAAJ&hl=en&oi=ao">Ran Cheng</a>,
<!--              <a href="https://www.chenghehust.com/">Cheng He</a>,-->
              <a href="https://scholar.google.com/citations?user=I28xMFQAAAAJ&hl=zh-CN">Cheng He</a>
              <br>
							<em>ICCV</em>, 2021 &nbsp
              <br>
              <a href="http://www.shihuahuang.cn/fapn">project page</a> /
              <a href="https://arxiv.org/abs/2108.07058">arXiv</a> /
              <a href="https://github.com/ShihuaHuang95/FaPN">code</a>
              <p></p>
              <p>FaPN is a simple yet effective top-down pyramidal architecture to generate multi-scale features for dense image prediction.
                It improves FPN's AP / mIoU by 1.5 - 2.6% on all tasks. It achieved 56.7% mIoU over ADE20k-150 when paired with MaskFormer.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/relativenas.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2009.06193">
                <papertitle>RelativeNAS: Relative Neural Architecture Search via Slow-Fast Learning</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=Dad3F3AAAAAJ&hl=en">Hao Tan</a>,
              <a href="https://scholar.google.com/citations?user=bjeIdlcAAAAJ&hl=en&oi=ao">Ran Cheng</a>,
              <strong>Shihua Huang</strong>,
              <a href="https://scholar.google.com/citations?user=I28xMFQAAAAJ&hl=zh-CN">Cheng He</a>
              <a href="">Changxiao Qiu</a>
              <a href="">Fan Yang</a>
              <a href="http://luoping.me/">Ping Luo</a>
              <br>
              <em>TNNLS </em>, 2021 &nbsp
              <br>
              <a href="https://arxiv.org/abs/2009.06193">arXiv</a> /
              <a href="https://github.com/EMI-Group/RelativeNAS">code</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gmoea.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9082904">
                <papertitle>Evolutionary Multi-Objective Optimization Driven by Generative Adversarial Networks (GANs)</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=I28xMFQAAAAJ&hl=zh-CN">Cheng He</a>,
              <strong>Shihua Huang</strong>,
              <a href="https://scholar.google.com/citations?user=bjeIdlcAAAAJ&hl=en&oi=ao">Ran Cheng</a>,
              <a href="https://scholar.google.com.sg/citations?user=LFngSp0AAAAJ&hl=en">Kay Chen Tan</a>
              <a href="https://scholar.google.com/citations?user=B5WAkz4AAAAJ&hl=en">Yaochu Jin</a>
              <br>
              <em>TCYB </em>, 2020 &nbsp
              <br>
              <a href="https://arxiv.org/abs/1910.04966">arXiv</a> /
              <a href="https://github.com/EMI-Group/GMOEA">code</a>
            </td>
          </tr>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
<!--            <td style="padding:2px;width:1%;vertical-align:middle"><img src="images/IEEE.jpg"></td>-->
            <td width="100%" valign="center">
              <strong>Reviewer:</strong> IEEE Trans. on Multi Media, IEEE Trans. on Cognitive and Developmental Systems,
              Applied Soft Computing, and Complex & Intelligent Systems.
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
