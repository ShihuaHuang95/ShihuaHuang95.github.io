
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>FaPN</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://jonbarron.info/mipnerf/img/rays_square.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="http://www.shihuahuang.cn/fapn"/>
    <meta property="og:title" content="FaPN" />
    <meta property="og:description" content="Project page for FaPN: Feature-aligned Pyramid Network for Dense Image Prediction." />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="FaPN" />
    <meta name="twitter:description" content="Project page for FaPN: Feature-aligned Pyramid Network for Dense Image Prediction." />
    <meta name="twitter:image" content="http://www.shihuahuang.cn/fapn/img/rays_square.png" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>FaPN</b>: Feature-aligned Pyramid Network for Dense Image Prediction</br>
                <small>
                    ICCV 2021
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="http://www.shihuahuang.cn">
                          Shihua Huang
                        </a>
                        </br>SUSTech
                    </li>
                    <li>
                        <a href="http://www.luzhichao.com/">
                            Zhichao Lu
                        </a>
                        </br>SUSTech
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=bjeIdlcAAAAJ&hl=en&oi=ao">
                          Ran Cheng
                        </a>
                        </br>SUSTech
                    </li>
                    <li>
                        <a href="https://www.chenghehust.com/">
                          Cheng He
                        </a>
                        </br>SUSTech
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2108.07058">
                            <image src="img/fapn_paper_image.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/ShihuaHuang95/FaPN">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="img/front_view.jpg" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    Recent advancements in deep neural networks have made remarkable leap-forwards in dense image prediction. However, the issue of feature alignment remains as neglected by most existing approaches for simplicity.
                    Direct pixel addition between upsampled and local features leads to feature maps with misaligned contexts that, in turn, translate to mis-classifications in prediction, especially on object boundaries.
                    In this paper, we propose a feature alignment module that learns transformation offsets of pixels to contextually align upsampled higher-level features; and another feature selection module to emphasize the lower-level features with rich spatial details.
                    We then integrate these two modules in a top-down pyramidal architecture and present the Feature-aligned Pyramid Network (FaPN). Extensive experimental evaluations on four dense prediction tasks and four datasets have demonstrated the efficacy of FaPN, yielding an overall improvement of 1.2 - 2.6 points in AP / mIoU over FPN when paired with Faster / Mask R-CNN.
                    In particular, our FaPN achieves the state-of-the-art of 56.7 % mIoU on ADE20K when integrated within MaskFormer.
                </p>
            </div>
        </div>


<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Boundary Prediction Analysis-->
<!--                </h3>-->
<!--                <p class="text-justify">-->
<!--                    Typical positional encoding (as used in Transformer networks and Neural Radiance Fields) maps a single point in space to a feature vector, where each element is generated by a sinusoid with an exponentially increasing frequency:-->
<!--                </p>-->
<!--                <p style="text-align:center;">-->
<!--                    <image src="img/pe_seq_eqn_pad.png" height="50px" class="img-responsive">-->
<!--                </p>-->
<!--                <video id="v0" width="100%" autoplay loop muted>-->
<!--                  <source src="img/pe_anim_horiz.mp4" type="video/mp4" />-->
<!--                </video>-->
<!--                <p class="text-justify">-->
<!--                    Here, we show how these feature vectors change as a function of a point moving in 1D space.-->
<!--                    <br><br>-->
<!--                    Our <em>integrated positional encoding</em> considers Gaussian <em>regions</em> of space, rather than infinitesimal points. This provides a natural way to input a "region" of space as query to a coordinate-based neural network, allowing the network to reason about sampling and aliasing. The expected value of each positional encoding component has a simple closed form:-->
<!--                </p>-->
<!--                <p style="text-align:center;">-->
<!--                    <image src="img/ipe_eqn_under_pad.png" height="30px" class="img-responsive">-->
<!--                </p>-->
<!--                <video id="v0" width="100%" autoplay loop muted>-->
<!--                  <source src="img/ipe_anim_horiz.mp4" type="video/mp4" />-->
<!--                </video>-->
<!--                <p class="text-justify">-->
<!--                    We can see that when considering a wider region, the higher frequency features automatically shrink toward zero, providing the network with lower-frequency inputs. As the region narrows, these features converge to the original positional encoding.-->
<!--                </p>-->
<!--            </div>-->
<!--        </div>-->
<!--            -->


<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Mip-NeRF-->
<!--                </h3>-->
<!--                <p class="text-justify">-->
<!--                    We use integrated positional encoding to train NeRF to generate anti-aliased renderings. Rather than casting an infinitesimal ray through each pixel, we instead cast a full 3D <em>cone</em>. For each queried point along a ray, we consider its associated 3D conical frustum. Two different cameras viewing the same point in space may result in vastly different conical frustums, as illustrated here in 2D:-->
<!--                </p>-->
<!--                <p style="text-align:center;">-->
<!--                    <image src="img/scales_toy.png" class="img-responsive" alt="scales">-->
<!--                </p>-->
<!--                <p class="text-justify">-->
<!--                    In order to pass this information through the NeRF network, we fit a multivariate Gaussian to the conical frustum and use the integrated positional encoding described above to create the input feature vector to the network. -->
<!--                </p>-->
<!--            </div>-->
<!--        </div>-->
<!--            -->


<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Results-->
<!--                </h3>-->
<!--                <p class="text-justify">-->
<!--                    We train NeRF and mip-NeRF on a dataset with images at four different resolutions. Normal NeRF (left) is not capable of learning to represent the same scene at multiple levels of detail, with blurring in close-up shots and aliasing in low resolution views, while mip-NeRF (right) both preserves sharp details in close-ups and correctly renders the zoomed-out images.-->
<!--                </p>                -->
<!--                <br>-->
<!--                <video id="v0" width="100%" autoplay loop muted controls>-->
<!--                  <source src="img/ship_sbs_path1.mp4" type="video/mp4" />-->
<!--                </video>-->
<!--                <video id="v0" width="100%" autoplay loop muted controls>-->
<!--                  <source src="img/chair_sbs_path1.mp4" type="video/mp4" />-->
<!--                </video>-->
<!--                <video id="v0" width="100%" autoplay loop muted controls>-->
<!--                  <source src="img/lego_sbs_path1.mp4" type="video/mp4" />-->
<!--                </video>-->
<!--                <video id="v0" width="100%" autoplay loop muted controls>-->
<!--                  <source src="img/mic_sbs_path1.mp4" type="video/mp4" />-->
<!--                </video>-->
<!--                <br><br>-->
<!--                <p class="text-justify">-->
<!--                    We can also manipulate the integrated positional encoding by using a larger or smaller radius than the true pixel footprint, exposing the continuous level of detail learned within a single network:-->
<!--                </p>     -->
<!--                <video id="v0" width="100%" autoplay loop muted controls>-->
<!--                  <source src="img/lego_radii_manip_slider_200p.mp4" type="video/mp4" />-->
<!--                </video>-->
<!--            </div>-->
<!--        </div>-->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Related links
                </h3>
                <p class="text-justify">
                    <a href="https://github.com/facebookresearch/detectron2">detectron2</a>
                    and <a href="https://github.com/facebookresearch/MaskFormer">MaskFormer</a>.
                </p>
            </div>
        </div>
        
            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
    @inproceedings{
      huang2021fapn,
      title={{FaPN}: Feature-aligned Pyramid Network for Dense Image Prediction},
      author={Shihua Huang and Zhichao Lu and Ran Cheng and Cheng He},
      booktitle={International Conference on Computer Vision (ICCV)},
      year={2021}
}</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    This work was supported by the National Natural Science Foundation of China (Grant No. 61903178, 61906081, and U20A20306)
                    and the Program for Guangdong Introducing Innovative and Enterpreneurial Teams (Grant No. 2017ZT07X386)
                    <br>
                The website template was borrowed from <a href="">Michaël Gharbi</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
